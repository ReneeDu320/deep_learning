{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReneeDu320/deep_learning/blob/main/densenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06mhFJuKCwhS"
      },
      "source": [
        "# DenseNet\n",
        "因为 ResNet 提出了跨层链接的思想，这直接影响了随后出现的卷积网络架构，其中最有名的就是 cvpr 2017 的 best paper，DenseNet。\n",
        "\n",
        "DenseNet 和 ResNet 不同在于 ResNet 是跨层求和，而 DenseNet 是跨层将特征在通道维度进行拼接，下面可以看看他们两者的图示\n",
        "\n",
        "![](https://gaussian37.github.io/assets/img/dl/concept/densenet/8.png)\n",
        "\n",
        "![](https://glassboxmedicine.files.wordpress.com/2020/12/vgg-resnet-googlenet-1.png?w=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MT4YprCwhW"
      },
      "source": [
        "第一张图是 ResNet，第二张图是 DenseNet，因为是在通道维度进行特征的拼接，所以底层的输出会保留进入所有后面的层，这能够更好的保证梯度的传播，同时能够使用低维的特征和高维的特征进行联合训练，能够得到更好的结果。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 L 层的网络，DenseNet共包含 $(l+1)l/2$个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别。\n",
        "\n",
        "\n",
        "\n",
        "如果用公式表示的话，传统的网络在 \n",
        "l层的输出为：\n",
        "\n",
        "$$x_l=H_l(x_{l−1})$$\n",
        "\n",
        "而对于ResNet，增加了来自上一层输入的identity函数：\n",
        "\n",
        "$$x_l=H_l(x_{l−1} ) + x_{l-1}$$\n",
        "\n",
        "在DenseNet中，会连接前面所有层作为输入：\n",
        "$$x_l=H_l([x_0,x_1,...,x_{l−1}])$$\n",
        "\n",
        "其中，上面的$H_l$代表是非线性转化函数（non-liear transformation），它是一个组合操作，其可能包括一系列的BN(Batch Normalization)，ReLU，Pooling及Conv操作。注意这里L层与L-1层之间可能实际上包含多个卷积层。\n",
        "\n",
        "DenseNet的前向过程如图3所示，可以更直观地理解其密集连接方式，比如 \n",
        "h_3的输入不仅包括来自 h_2的 x_2，还包括前面两层的 x_1和 x_2，它们是在channel维度上连接在一起的。\n",
        "![DenseNet的前向过程](https://pic2.zhimg.com/80/v2-0a9db078f505b469973974aee9c27605_1440w.webp)\n",
        "\n",
        "CNN网络一般要经过Pooling或者stride>1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。DenseNet的网路结构，各个DenseBlock之间通过Transition连接在一起。\n",
        "\n",
        "![](https://pic4.zhimg.com/80/v2-0b28a49f274da9bd8dec2dccddf1ec53_1440w.webp)\n",
        "\n",
        "在DenseBlock中，各个层的特征图大小一致，可以在channel维度上连接。DenseBlock中的非线性组合函数 H 采用的是BN+ReLU+3x3 Conv的结构，如图6所示。另外值得注意的一点是，与ResNet不同，所有DenseBlock中各个层卷积之后均输出 k个特征图，即得到的特征图的channel数为 k，或者说采用 k个卷积核。 \n",
        "k在DenseNet称为growth rate，这是一个超参数。一般情况下使用较小的 k（比如12），就可以得到较佳的性能。假定输入层的特征图的channel数为k_0，那么 \n",
        "l层输入的channel数为$k_0+k_{l-1}$，因此随着层数增加，尽管k设定得较小，DenseBlock的输入会非常多，不过这是由于特征重用所造成的，每个层仅有 \n",
        "k个特征是自己独有的。\n",
        "\n",
        "\n",
        "综合来看，DenseNet的优势主要体现在以下几个方面：\n",
        "\n",
        "- 由于密集连接方式，DenseNet提升了梯度的反向传播，使得网络更容易训练。由于每层可以直达最后的误差信号，实现了隐式的“deep supervision”；\n",
        "- 参数更小且计算更高效，这有点违反直觉，由于DenseNet是通过concat特征来实现短路连接，实现了特征重用，并且采用较小的growth rate，每个层所独有的特征图是比较小的；\n",
        "- 由于特征复用，最后的分类器使用了低级特征。"
      ],
      "metadata": {
        "id": "9RhYK-RSJWa-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tkbnMEbCwhX"
      },
      "source": [
        "DenseNet 主要由 dense block 构成，下面我们来实现一个 densen block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.113030Z",
          "start_time": "2017-12-22T15:38:30.612922Z"
        },
        "collapsed": true,
        "id": "gBv0FnhpCwhX"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiuJa5yfCwhZ"
      },
      "source": [
        "首先定义一个卷积块，这个卷积块的顺序是 bn -> relu -> conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.121249Z",
          "start_time": "2017-12-22T15:38:31.115369Z"
        },
        "collapsed": true,
        "id": "HTcnvLlGCwhZ"
      },
      "outputs": [],
      "source": [
        "def conv_block(in_channel, out_channel):\n",
        "    layer = nn.Sequential(\n",
        "        nn.BatchNorm2d(in_channel),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False)\n",
        "    )\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8USws68wCwha"
      },
      "source": [
        "dense block 将每次的卷积的输出称为 `growth_rate`，因为如果输入是 `in_channel`，有 n 层，那么输出就是 `in_channel + n * growh_rate`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.145274Z",
          "start_time": "2017-12-22T15:38:31.123363Z"
        },
        "collapsed": true,
        "id": "E9135PILCwhb"
      },
      "outputs": [],
      "source": [
        "class dense_block(nn.Module):\n",
        "    def __init__(self, in_channel, growth_rate, num_layers):\n",
        "        super(dense_block, self).__init__()\n",
        "        block = []\n",
        "        channel = in_channel\n",
        "        for i in range(num_layers):\n",
        "            block.append(conv_block(channel, growth_rate))\n",
        "            channel += growth_rate\n",
        "            \n",
        "        self.net = nn.Sequential(*block)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        for layer in self.net:\n",
        "            out = layer(x)\n",
        "            x = torch.cat((out, x), dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8q0k_drCwhc"
      },
      "source": [
        "我们验证一下输出的 channel 是否正确"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.213632Z",
          "start_time": "2017-12-22T15:38:31.147196Z"
        },
        "id": "_Wl81QFYCwhc",
        "outputId": "08537dce-7971-4b30-db97-9fb954fc9720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: 3 x 96 x 96\n",
            "output shape: 39 x 96 x 96\n"
          ]
        }
      ],
      "source": [
        "test_net = dense_block(3, 12, 3)\n",
        "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
        "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
        "test_y = test_net(test_x)\n",
        "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZrj6ElwCwhe"
      },
      "source": [
        "除了 dense block，DenseNet 中还有一个模块叫过渡层（transition block），因为 DenseNet 会不断地对维度进行拼接， 所以当层数很高的时候，输出的通道数就会越来越大，参数和计算量也会越来越大，为了避免这个问题，需要引入过渡层将输出通道降低下来，同时也将输入的长宽减半，这个过渡层可以使用 1 x 1 的卷积"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.222120Z",
          "start_time": "2017-12-22T15:38:31.215770Z"
        },
        "collapsed": true,
        "id": "9wl2ZeVMCwhf"
      },
      "outputs": [],
      "source": [
        "def transition(in_channel, out_channel):\n",
        "    trans_layer = nn.Sequential(\n",
        "        nn.BatchNorm2d(in_channel),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(in_channel, out_channel, 1),\n",
        "        nn.AvgPool2d(2, 2)\n",
        "    )\n",
        "    return trans_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J8hDTbZCwhg"
      },
      "source": [
        "验证一下过渡层是否正确"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.234846Z",
          "start_time": "2017-12-22T15:38:31.224078Z"
        },
        "id": "NWBlQDb_Cwhg",
        "outputId": "2a46629d-e31d-4320-ddb3-5b09b86296c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: 3 x 96 x 96\n",
            "output shape: 12 x 48 x 48\n"
          ]
        }
      ],
      "source": [
        "test_net = transition(3, 12)\n",
        "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
        "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
        "test_y = test_net(test_x)\n",
        "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZuqElPUCwhi"
      },
      "source": [
        "最后我们定义 DenseNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.318822Z",
          "start_time": "2017-12-22T15:38:31.236857Z"
        },
        "collapsed": true,
        "id": "M2Qsc40LCwhj"
      },
      "outputs": [],
      "source": [
        "class densenet(nn.Module):\n",
        "    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]):\n",
        "        super(densenet, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channel, 64, 7, 2, 3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(3, 2, padding=1)\n",
        "        )\n",
        "        \n",
        "        channels = 64\n",
        "        block = []\n",
        "        for i, layers in enumerate(block_layers):\n",
        "            block.append(dense_block(channels, growth_rate, layers))\n",
        "            channels += layers * growth_rate\n",
        "            if i != len(block_layers) - 1:\n",
        "                block.append(transition(channels, channels // 2)) # 通过 transition 层将大小减半，通道数减半\n",
        "                channels = channels // 2\n",
        "        \n",
        "        self.block2 = nn.Sequential(*block)\n",
        "        self.block2.add_module('bn', nn.BatchNorm2d(channels))\n",
        "        self.block2.add_module('relu', nn.ReLU(True))\n",
        "        self.block2.add_module('avg_pool', nn.AvgPool2d(3))\n",
        "        \n",
        "        self.classifier = nn.Linear(channels, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        \n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:31.654182Z",
          "start_time": "2017-12-22T15:38:31.320788Z"
        },
        "id": "VXAl6oH9Cwhk",
        "outputId": "38a9a9b6-8730-4cae-afce-72ef427a59bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output: torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "test_net = densenet(3, 10)\n",
        "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
        "test_y = test_net(test_x)\n",
        "print('output: {}'.format(test_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T15:38:32.894729Z",
          "start_time": "2017-12-22T15:38:31.656356Z"
        },
        "collapsed": true,
        "id": "IHdftGNfCwhl"
      },
      "outputs": [],
      "source": [
        "from utils import train\n",
        "\n",
        "def data_tf(x):\n",
        "    x = x.resize((96, 96), 2) # 将图片放大到 96 x 96\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
        "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "     \n",
        "train_set = CIFAR10('./data', train=True, transform=data_tf)\n",
        "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_set = CIFAR10('./data', train=False, transform=data_tf)\n",
        "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "\n",
        "net = densenet(3, 10)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T16:15:38.168095Z",
          "start_time": "2017-12-22T15:38:32.896735Z"
        },
        "id": "kInyxdXYCwhl",
        "outputId": "f9ddb032-d125-48b3-ce9c-a35977ac4467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Train Loss: 1.374316, Train Acc: 0.507972, Valid Loss: 1.203217, Valid Acc: 0.572884, Time 00:01:44\n",
            "Epoch 1. Train Loss: 0.912924, Train Acc: 0.681506, Valid Loss: 1.555908, Valid Acc: 0.492286, Time 00:01:50\n",
            "Epoch 2. Train Loss: 0.701387, Train Acc: 0.755794, Valid Loss: 0.815147, Valid Acc: 0.718354, Time 00:01:49\n",
            "Epoch 3. Train Loss: 0.575985, Train Acc: 0.800911, Valid Loss: 0.696013, Valid Acc: 0.759494, Time 00:01:50\n",
            "Epoch 4. Train Loss: 0.479812, Train Acc: 0.836957, Valid Loss: 1.013879, Valid Acc: 0.676226, Time 00:01:51\n",
            "Epoch 5. Train Loss: 0.402165, Train Acc: 0.861413, Valid Loss: 0.674512, Valid Acc: 0.778481, Time 00:01:50\n",
            "Epoch 6. Train Loss: 0.334593, Train Acc: 0.888247, Valid Loss: 0.647112, Valid Acc: 0.791634, Time 00:01:50\n",
            "Epoch 7. Train Loss: 0.278181, Train Acc: 0.907149, Valid Loss: 0.773517, Valid Acc: 0.756527, Time 00:01:51\n",
            "Epoch 8. Train Loss: 0.227948, Train Acc: 0.922714, Valid Loss: 0.654399, Valid Acc: 0.800237, Time 00:01:49\n",
            "Epoch 9. Train Loss: 0.181156, Train Acc: 0.940157, Valid Loss: 1.179013, Valid Acc: 0.685225, Time 00:01:50\n",
            "Epoch 10. Train Loss: 0.151305, Train Acc: 0.950208, Valid Loss: 0.630000, Valid Acc: 0.807951, Time 00:01:50\n",
            "Epoch 11. Train Loss: 0.118433, Train Acc: 0.961077, Valid Loss: 1.247253, Valid Acc: 0.703323, Time 00:01:52\n",
            "Epoch 12. Train Loss: 0.094127, Train Acc: 0.969789, Valid Loss: 1.230697, Valid Acc: 0.723101, Time 00:01:51\n",
            "Epoch 13. Train Loss: 0.086181, Train Acc: 0.972047, Valid Loss: 0.904135, Valid Acc: 0.769284, Time 00:01:50\n",
            "Epoch 14. Train Loss: 0.064248, Train Acc: 0.980359, Valid Loss: 1.665002, Valid Acc: 0.624209, Time 00:01:51\n",
            "Epoch 15. Train Loss: 0.054932, Train Acc: 0.982996, Valid Loss: 0.927216, Valid Acc: 0.774723, Time 00:01:51\n",
            "Epoch 16. Train Loss: 0.043503, Train Acc: 0.987272, Valid Loss: 1.574383, Valid Acc: 0.707377, Time 00:01:52\n",
            "Epoch 17. Train Loss: 0.047615, Train Acc: 0.985154, Valid Loss: 0.987781, Valid Acc: 0.770471, Time 00:01:51\n",
            "Epoch 18. Train Loss: 0.039813, Train Acc: 0.988012, Valid Loss: 2.248944, Valid Acc: 0.631824, Time 00:01:50\n",
            "Epoch 19. Train Loss: 0.030183, Train Acc: 0.991168, Valid Loss: 0.887785, Valid Acc: 0.795392, Time 00:01:51\n"
          ]
        }
      ],
      "source": [
        "train(net, train_data, test_data, 20, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72j7EOunCwhm"
      },
      "source": [
        "DenseNet 将残差连接改为了特征拼接，使得网络有了更稠密的连接"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#首先实现DenseBlock中的内部结构，这里是BN+ReLU+1x1 Conv+BN+ReLU+3x3 Conv结构，最后也加入dropout层以用于训练过程。\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    \"\"\"Basic unit of DenseBlock (using bottleneck layer) \"\"\"\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module(\"norm1\", nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module(\"relu1\", nn.ReLU(inplace=True))\n",
        "        self.add_module(\"conv1\", nn.Conv2d(num_input_features, bn_size*growth_rate,\n",
        "                                           kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module(\"norm2\", nn.BatchNorm2d(bn_size*growth_rate))\n",
        "        self.add_module(\"relu2\", nn.ReLU(inplace=True))\n",
        "        self.add_module(\"conv2\", nn.Conv2d(bn_size*growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1, bias=False))\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    \"\"\"DenseBlock\"\"\"\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features+i*growth_rate, growth_rate, bn_size,\n",
        "                                drop_rate)\n",
        "            self.add_module(\"denselayer%d\" % (i+1,), layer)\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    \"\"\"Transition layer between two adjacent DenseBlock\"\"\"\n",
        "    def __init__(self, num_input_feature, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module(\"norm\", nn.BatchNorm2d(num_input_feature))\n",
        "        self.add_module(\"relu\", nn.ReLU(inplace=True))\n",
        "        self.add_module(\"conv\", nn.Conv2d(num_input_feature, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module(\"pool\", nn.AvgPool2d(2, stride=2))\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \"DenseNet-BC model\"\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64,\n",
        "                 bn_size=4, compression_rate=0.5, drop_rate=0, num_classes=1000):\n",
        "        \"\"\"\n",
        "        :param growth_rate: (int) number of filters used in DenseLayer, `k` in the paper\n",
        "        :param block_config: (list of 4 ints) number of layers in each DenseBlock\n",
        "        :param num_init_features: (int) number of filters in the first Conv2d\n",
        "        :param bn_size: (int) the factor using in the bottleneck layer\n",
        "        :param compression_rate: (float) the compression rate used in Transition Layer\n",
        "        :param drop_rate: (float) the drop rate after each DenseLayer\n",
        "        :param num_classes: (int) number of classes for classification\n",
        "        \"\"\"\n",
        "        super(DenseNet, self).__init__()\n",
        "        # first Conv2d\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            (\"conv0\", nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            (\"norm0\", nn.BatchNorm2d(num_init_features)),\n",
        "            (\"relu0\", nn.ReLU(inplace=True)),\n",
        "            (\"pool0\", nn.MaxPool2d(3, stride=2, padding=1))\n",
        "        ]))\n",
        "\n",
        "        # DenseBlock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers, num_features, bn_size, growth_rate, drop_rate)\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "            num_features += num_layers*growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                transition = _Transition(num_features, int(num_features*compression_rate))\n",
        "                self.features.add_module(\"transition%d\" % (i + 1), transition)\n",
        "                num_features = int(num_features * compression_rate)\n",
        "\n",
        "        # final bn+ReLU\n",
        "        self.features.add_module(\"norm5\", nn.BatchNorm2d(num_features))\n",
        "        self.features.add_module(\"relu5\", nn.ReLU(inplace=True))\n",
        "\n",
        "        # classification layer\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # params initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        out = F.avg_pool2d(features, 7, stride=1).view(features.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "#选择不同网络参数，就可以实现不同深度的DenseNet，这里实现DenseNet-121网络，而且Pytorch提供了预训练好的网络参数：\n",
        "def densenet121(pretrained=False, **kwargs):\n",
        "    \"\"\"DenseNet121\"\"\"\n",
        "    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                     **kwargs)\n",
        "\n",
        "    if pretrained:\n",
        "        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n",
        "        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "        # They are also in the checkpoints in model_urls. This pattern is used\n",
        "        # to find such keys.\n",
        "        pattern = re.compile(\n",
        "            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "        state_dict = model_zoo.load_url(model_urls['densenet121'])\n",
        "        for key in list(state_dict.keys()):\n",
        "            res = pattern.match(key)\n",
        "            if res:\n",
        "                new_key = res.group(1) + res.group(2)\n",
        "                state_dict[new_key] = state_dict[key]\n",
        "                del state_dict[key]\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "#test\n",
        "densenet = densenet121(pretrained=True)\n",
        "densenet.eval()\n",
        "\n",
        "img = Image.open(\"./images/cat.jpg\")\n",
        "\n",
        "trans_ops = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "images = trans_ops(img).view(-1, 3, 224, 224)\n",
        "outputs = densenet(images)\n",
        "\n",
        "_, predictions = outputs.topk(5, dim=1)\n",
        "\n",
        "labels = list(map(lambda s: s.strip(), open(\"./data/imagenet/synset_words.txt\").readlines()))\n",
        "for idx in predictions.numpy()[0]:\n",
        "    print(\"Predicted labels:\", labels[idx])"
      ],
      "metadata": {
        "id": "zfbF6BllOHeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
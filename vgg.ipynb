{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReneeDu320/deep_learning/blob/main/vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpJKRn4yam4t"
      },
      "source": [
        "# VGG\n",
        "计算机视觉是一直深度学习的主战场，从这里我们将接触到近几年非常流行的卷积网络结构，网络结构由浅变深，参数越来越多，网络有着更多的跨层链接，首先我们先介绍一个数据集 cifar10，我们将以此数据集为例介绍各种卷积网络的结构。\n",
        "\n",
        "## CIFAR 10\n",
        "cifar 10 这个数据集一共有 50000 张训练集，10000 张测试集，两个数据集里面的图片都是 png 彩色图片，图片大小是 32 x 32 x 3，一共是 10 分类问题，分别为飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。这个数据集是对网络性能测试一个非常重要的指标，可以说如果一个网络在这个数据集上超过另外一个网络，那么这个网络性能上一定要比另外一个网络好，目前这个数据集最好的结果是 95% 左右的测试集准确率。\n",
        "\n",
        "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpjxxq7wcj30db0ae7ag.jpg)\n",
        "\n",
        "你能用肉眼对这些图片进行分类吗？\n",
        "\n",
        "cifar 10 已经被 pytorch 内置了，使用非常方便，只需要调用 `torchvision.datasets.CIFAR10` 就可以了"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0dRIzh9am4w"
      },
      "source": [
        "## VGGNet\n",
        "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。\n",
        "\n",
        "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
        "\n",
        "![](https://neurohive.io/wp-content/uploads/2018/11/vgg16.png)\n",
        "\n",
        "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
        "\n",
        "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次，下面我们照着这个结构来写一写"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:51.296457Z",
          "start_time": "2017-12-22T09:01:50.883050Z"
        },
        "collapsed": true,
        "id": "uz-2OLNGam4x"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6zJWhx8am4z"
      },
      "source": [
        "我们可以定义一个 vgg 的 block，传入三个参数，第一个是模型层数，第二个是输入的通道数，第三个是输出的通道数，第一层卷积接受的输入通道就是图片输入的通道数，然后输出最后的输出通道数，后面的卷积接受的通道数就是最后的输出通道数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:51.312500Z",
          "start_time": "2017-12-22T09:01:51.298777Z"
        },
        "collapsed": true,
        "id": "0dWLMpGfam40"
      },
      "outputs": [],
      "source": [
        "def vgg_block(num_convs, in_channels, out_channels):\n",
        "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义第一层\n",
        "    \n",
        "    for i in range(num_convs-1): # 定义后面的很多层\n",
        "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
        "        net.append(nn.ReLU(True))\n",
        "        \n",
        "    net.append(nn.MaxPool2d(2, 2)) # 定义池化层\n",
        "    return nn.Sequential(*net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS3sPKY3am40"
      },
      "source": [
        "我们可以将模型打印出来看看结构"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T08:20:40.819497Z",
          "start_time": "2017-12-22T08:20:40.808853Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocENnBMBam41",
        "outputId": "dff91e7b-802f-46b8-c972-7a4d885e9f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (5): ReLU(inplace=True)\n",
            "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "block_demo = vgg_block(3, 64, 128)\n",
        "print(block_demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T07:52:04.632406Z",
          "start_time": "2017-12-22T07:52:02.381987Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4l_Swhham42",
        "outputId": "029c2d82-83e3-47b9-8ad4-f85ffc9140a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 150, 150])\n"
          ]
        }
      ],
      "source": [
        "# 首先定义输入为 (1, 64, 300, 300)\n",
        "input_demo = Variable(torch.zeros(1, 64, 300, 300))\n",
        "output_demo = block_demo(input_demo)\n",
        "print(output_demo.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suuJxtjbam43"
      },
      "source": [
        "可以看到输出就变为了 (1, 128, 150, 150)，可以看到经过了这一个 vgg block，输入大小被减半，通道数变成了 128\n",
        "\n",
        "下面我们定义一个函数对这个 vgg block 进行堆叠"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:54.497712Z",
          "start_time": "2017-12-22T09:01:54.489255Z"
        },
        "collapsed": true,
        "id": "J-fXPytEam43"
      },
      "outputs": [],
      "source": [
        "def vgg_stack(num_convs, channels):\n",
        "    net = []\n",
        "    for n, c in zip(num_convs, channels):\n",
        "        in_c = c[0]\n",
        "        out_c = c[1]\n",
        "        net.append(vgg_block(n, in_c, out_c))\n",
        "    return nn.Sequential(*net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KFgSuB0am44"
      },
      "source": [
        "作为实例，我们定义一个稍微简单一点的 vgg 结构，其中有 8 个卷积层"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:55.149378Z",
          "start_time": "2017-12-22T09:01:55.041923Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orfsan5dam44",
        "outputId": "57c9b20b-4b15-4eca-c936-e00115c2b580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\n",
        "print(vgg_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srU9gDOkam45"
      },
      "source": [
        "我们可以看到网络结构中有个 5 个 最大池化，说明图片的大小会减少 5 倍，我们可以验证一下，输入一张 256 x 256 的图片看看结果是什么"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T08:52:44.049650Z",
          "start_time": "2017-12-22T08:52:43.431478Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yhCQEO4am45",
        "outputId": "bad84759-4a0a-4e2d-df32-9c9b9beb7236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "test_x = Variable(torch.zeros(1, 3, 256, 256))\n",
        "test_y = vgg_net(test_x)\n",
        "print(test_y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZEEAA55am46"
      },
      "source": [
        "可以看到图片减小了 $2^5$ 倍，最后再加上几层全连接，就能够得到我们想要的分类输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:57.323034Z",
          "start_time": "2017-12-22T09:01:57.306864Z"
        },
        "collapsed": true,
        "id": "G_FHM1aJam46"
      },
      "outputs": [],
      "source": [
        "class vgg(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(vgg, self).__init__()\n",
        "        self.feature = vgg_net\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 100),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(100, 10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmi_VwxNam47"
      },
      "source": [
        "然后我们可以训练我们的模型看看在 cifar10 上的效果"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def get_acc(output, label):\n",
        "    total = output.shape[0]\n",
        "    _, pred_label = output.max(1)\n",
        "    num_correct = (pred_label == label).sum().data\n",
        "    return num_correct / total\n",
        "\n",
        "\n",
        "def train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n",
        "    if torch.cuda.is_available():\n",
        "        net = net.cuda()\n",
        "    prev_time = datetime.now()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        net = net.train()\n",
        "        for im, label in train_data:\n",
        "            if torch.cuda.is_available():\n",
        "                im = Variable(im.cuda())  # (bs, 3, h, w)\n",
        "                label = Variable(label.cuda())  # (bs, h, w)\n",
        "            else:\n",
        "                im = Variable(im)\n",
        "                label = Variable(label)\n",
        "            # forward\n",
        "            output = net(im)\n",
        "            loss = criterion(output, label)\n",
        "            # backward\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.data\n",
        "            train_acc += get_acc(output, label)\n",
        "\n",
        "        cur_time = datetime.now()\n",
        "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
        "        m, s = divmod(remainder, 60)\n",
        "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
        "        if valid_data is not None:\n",
        "            valid_loss = 0\n",
        "            valid_acc = 0\n",
        "            net = net.eval()\n",
        "            for im, label in valid_data:\n",
        "                if torch.cuda.is_available():\n",
        "                    im = Variable(im.cuda(), volatile=True)\n",
        "                    label = Variable(label.cuda(), volatile=True)\n",
        "                else:\n",
        "                    im = Variable(im, volatile=True)\n",
        "                    label = Variable(label, volatile=True)\n",
        "                output = net(im)\n",
        "                loss = criterion(output, label)\n",
        "                valid_loss += loss.data\n",
        "                valid_acc += get_acc(output, label)\n",
        "            epoch_str = (\n",
        "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
        "                % (epoch, train_loss / len(train_data),\n",
        "                   train_acc / len(train_data), valid_loss / len(valid_data),\n",
        "                   valid_acc / len(valid_data)))\n",
        "        else:\n",
        "            epoch_str = (\"Epoch %d. Train Loss: %f, Train Acc: %f, \" %\n",
        "                         (epoch, train_loss / len(train_data),\n",
        "                          train_acc / len(train_data)))\n",
        "        prev_time = cur_time\n",
        "        print(epoch_str + time_str)\n",
        "\n",
        "\n",
        "def conv3x3(in_channel, out_channel, stride=1):\n",
        "    return nn.Conv2d(\n",
        "        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class residual_block(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, same_shape=True):\n",
        "        super(residual_block, self).__init__()\n",
        "        self.same_shape = same_shape\n",
        "        stride = 1 if self.same_shape else 2\n",
        "\n",
        "        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.conv2 = conv3x3(out_channel, out_channel)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        if not self.same_shape:\n",
        "            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out), True)\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.bn2(out), True)\n",
        "\n",
        "        if not self.same_shape:\n",
        "            x = self.conv3(x)\n",
        "        return F.relu(x + out, True)\n",
        "\n",
        "\n",
        "class resnet(nn.Module):\n",
        "    def __init__(self, in_channel, num_classes, verbose=False):\n",
        "        super(resnet, self).__init__()\n",
        "        self.verbose = verbose\n",
        "\n",
        "        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            residual_block(64, 128, False), residual_block(128, 128))\n",
        "\n",
        "        self.block4 = nn.Sequential(\n",
        "            residual_block(128, 256, False), residual_block(256, 256))\n",
        "\n",
        "        self.block5 = nn.Sequential(\n",
        "            residual_block(256, 512, False),\n",
        "            residual_block(512, 512), nn.AvgPool2d(3))\n",
        "\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        if self.verbose:\n",
        "            print('block 1 output: {}'.format(x.shape))\n",
        "        x = self.block2(x)\n",
        "        if self.verbose:\n",
        "            print('block 2 output: {}'.format(x.shape))\n",
        "        x = self.block3(x)\n",
        "        if self.verbose:\n",
        "            print('block 3 output: {}'.format(x.shape))\n",
        "        x = self.block4(x)\n",
        "        if self.verbose:\n",
        "            print('block 4 output: {}'.format(x.shape))\n",
        "        x = self.block5(x)\n",
        "        if self.verbose:\n",
        "            print('block 5 output: {}'.format(x.shape))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qDp6Vjikb8Wi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:01:59.921373Z",
          "start_time": "2017-12-22T09:01:58.709531Z"
        },
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtP6sRHeam47",
        "outputId": "a17a950f-9046-4214-a6c8-840092890049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 15864803.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "def data_tf(x):\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
        "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
        "    x = torch.from_numpy(x)\n",
        "    return x\n",
        "     \n",
        "train_set = CIFAR10('./data', download=True,train=True, transform=data_tf)\n",
        "train_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_set = CIFAR10('./data', download=True,train=False, transform=data_tf)\n",
        "test_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "\n",
        "net = vgg()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-12-22T09:12:46.868967Z",
          "start_time": "2017-12-22T09:01:59.924086Z"
        },
        "id": "Ztw4jtORam47"
      },
      "outputs": [],
      "source": [
        "train(net, train_data, test_data, 20, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcOYqP7bam48"
      },
      "source": [
        "可以看到，跑完 20 次，vgg 能在 cifar 10 上取得 76% 左右的测试准确率"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3hH55qwFvWL9bJfCYIukr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "msG8QzwHBWZ_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch is similar to numpy but can run on GPU"
      ],
      "metadata": {
        "id": "tefEeIe5Bdlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建一个 numpy ndarray\n",
        "numpy_tensor = np.random.randn(10, 20) # Uniformly distributed floats over [0, 1) and normally distributed\n",
        "numpy_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66uFBWK1BbMO",
        "outputId": "eb75ec4b-f9c5-453c-ec8f-75cd7a8f7f5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.06191989, -0.41222913, -0.78557398,  0.64694303, -1.47365569,\n",
              "        -1.29847439,  0.06243896, -0.6116409 , -0.60722276,  0.96338425,\n",
              "        -1.35071806, -2.45575326, -2.04397362, -0.19189195, -0.24039745,\n",
              "         0.23219301, -0.59517626,  0.83291552, -0.45072781, -0.18976192],\n",
              "       [-0.86372197, -1.73553531, -0.55897759, -1.67242617,  2.46670261,\n",
              "         1.08155503, -0.38750959,  0.3954579 , -0.02826794,  0.98628238,\n",
              "         1.43753395,  0.70333173, -0.22447596, -0.0862116 ,  0.49030129,\n",
              "         1.23622065, -0.36132387, -0.30580081, -0.60276072,  0.32513557],\n",
              "       [ 0.97429289,  0.2173625 ,  0.47856582, -1.0435808 , -0.54770982,\n",
              "        -0.89551609, -0.63466349,  1.23574382,  1.15480442, -0.59924094,\n",
              "         0.96132239,  0.34528879,  1.39903263,  1.71049909,  0.37806151,\n",
              "        -0.08721293, -1.55909763,  0.18827277,  1.63383547, -1.1794518 ],\n",
              "       [ 1.81680978, -1.15031558, -0.26672517,  1.91356811, -0.00275361,\n",
              "        -1.26786037, -0.7369151 ,  1.26087088,  0.80131659,  0.65271414,\n",
              "         1.07105432, -0.69427379, -1.60366197, -0.57889584, -0.22826501,\n",
              "        -0.09230033,  2.05235148, -0.57707781,  0.50124011,  0.26413557],\n",
              "       [ 0.64364874, -0.17656089, -0.40441468, -0.94416254, -0.27179367,\n",
              "         1.12487142,  0.46441156, -0.68930547, -0.48951092, -0.07347723,\n",
              "        -0.20649712, -0.8159343 ,  0.40436441, -0.05206497,  0.07338668,\n",
              "        -0.82433511, -0.62154019,  0.32464186, -2.30289126,  1.8407035 ],\n",
              "       [ 1.60950678, -1.4858005 ,  0.15661547,  1.13514716,  0.17053403,\n",
              "         0.06976454, -0.43469493, -1.57525694, -1.40926636, -0.32778016,\n",
              "        -0.40304539,  0.51252061,  0.52395764,  0.73408602,  0.30345648,\n",
              "        -0.99023888, -0.21024159, -0.63282929,  1.21808627, -1.20060222],\n",
              "       [-0.89995646,  0.0983537 , -0.85232149,  0.92044185,  0.86296431,\n",
              "         0.84231312, -0.86113544,  1.1218597 ,  0.33784361, -0.71927487,\n",
              "        -0.24649184, -1.39221693,  0.4303116 , -0.57619822, -1.42634867,\n",
              "        -1.25884856,  0.34407186,  0.83605815,  0.26413103, -0.3376274 ],\n",
              "       [ 0.99987453, -0.61593897,  0.42004818,  1.38723664, -0.12048345,\n",
              "         0.07800458, -0.13096965,  0.32294222, -0.4640306 ,  0.99834507,\n",
              "        -0.74732557,  2.70359807, -0.59283236,  0.37083637,  0.39691138,\n",
              "         0.84145049, -0.0738314 , -1.17992543, -0.49308418,  0.75979432],\n",
              "       [-0.60828205,  1.3694784 , -2.17831065, -0.40303877,  1.45244382,\n",
              "         0.73096738,  1.78115694, -1.22041722,  0.7225477 ,  0.03544802,\n",
              "        -1.7575667 , -0.84989459,  0.23811398,  0.05462343, -2.70926108,\n",
              "        -1.15604478, -0.31269898, -0.53706849, -0.19245506, -1.0581613 ],\n",
              "       [-0.13539522,  0.90669992,  0.25125872,  1.50110115,  1.64694394,\n",
              "        -0.01109791,  2.00583354, -0.18224261, -1.0367238 ,  2.32786688,\n",
              "         2.47867598, -0.42362187,  2.1561339 , -1.18378679, -1.5419304 ,\n",
              "         0.16768759, -0.25764183, -0.87103502, -1.04577347, -0.08160369]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer ndarray to tensor in 2 ways\n",
        "pytorch_tensor1 = torch.Tensor(numpy_tensor)\n",
        "pytorch_tensor2 = torch.from_numpy(numpy_tensor)\n",
        "\n",
        "# reverse\n",
        "# 如果 pytorch tensor 在 cpu 上\n",
        "numpy_array = pytorch_tensor1.numpy()\n",
        "\n",
        "# 如果 pytorch tensor 在 gpu 上\n",
        "numpy_array = pytorch_tensor1.cpu().numpy()"
      ],
      "metadata": {
        "id": "KqFOSY-bBhcV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put tensor on GPU\n",
        "# 第一种方式是定义 cuda 数据类型\n",
        "dtype = torch.cuda.FloatTensor # 定义默认 GPU 的 数据类型\n",
        "gpu_tensor = torch.randn(10, 20).type(dtype)\n",
        "\n",
        "# 第二种方式更简单，推荐使用\n",
        "gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上\n",
        "gpu_tensor = torch.randn(10, 20).cuda(1) # 将 tensor 放到第二个 GPU 上\n",
        "\n",
        "\n",
        "# put it back to cpu\n",
        "cpu_tensor = gpu_tensor.cpu()"
      ],
      "metadata": {
        "id": "lsVPI4RuB1qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可以通过下面两种方式得到 tensor 的大小\n",
        "print(pytorch_tensor1.shape)\n",
        "print(pytorch_tensor1.size())\n",
        "\n",
        "# 得到 tensor 的数据类型\n",
        "print(pytorch_tensor1.type())\n",
        "\n",
        "# 得到 tensor 的维度\n",
        "print(pytorch_tensor1.dim())\n",
        "\n",
        "# 得到 tensor 的所有元素个数\n",
        "print(pytorch_tensor1.numel())"
      ],
      "metadata": {
        "id": "v1oWyX64CJDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch operations"
      ],
      "metadata": {
        "id": "YeA0Cc_CCaeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2)\n",
        "print(x) # 这是一个float tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbtkHvJlCbqv",
        "outputId": "b1f366e1-5288-4e5e-a312-ea001e5eabf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 将其转化为整形\n",
        "x = x.long()\n",
        "# x = x.type(torch.LongTensor)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vdGLfPHCcNb",
        "outputId": "cdfd5569-f825-4734-d02a-9482345e8754"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1],\n",
            "        [1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 再将其转回 float\n",
        "x = x.float()\n",
        "# x = x.type(torch.FloatTensor)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht_ot3rtCfOZ",
        "outputId": "c42b85bc-feeb-4ce6-df65-0d52c76074df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(4, 3)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBaxt0hVChzh",
        "outputId": "170d59f6-34e1-4d2a-d2a5-904a985cc487"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1183,  0.9595, -0.2493],\n",
            "        [ 0.3594,  0.3995, -1.5957],\n",
            "        [-1.5453,  0.6201, -0.7023],\n",
            "        [ 0.4077,  0.7170, -0.5429]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 沿着行取最大值\n",
        "max_value, max_idx = torch.max(x, dim=1)\n",
        "# 每一行的最大值\n",
        "max_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-sUbiMvCkJH",
        "outputId": "7b205b3a-e046-4dda-8d3f-46dfa2f75aef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9595, 0.3995, 0.6201, 0.7170])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 每一行最大值的下标\n",
        "max_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbJKB1ZFCmaM",
        "outputId": "d20e03d8-0f4a-47f0-8e51-6640362fb6a2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 沿着行对 x 求和\n",
        "sum_x = torch.sum(x, dim=1)\n",
        "print(sum_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ7XD5PHCrJ_",
        "outputId": "dfda1ece-4702-47c6-fda6-07dbc5c34436"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.4082, -0.8368, -1.6275,  0.5817])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 增加维度或者减少维度\n",
        "print(x.shape)\n",
        "x = x.unsqueeze(0) # 在第一维增加\n",
        "print(x.shape)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04_Hu1ZtCtIJ",
        "outputId": "a5fe12ff-cd47-4530-cab7-42a089a51e90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "torch.Size([1, 4, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.1183,  0.9595, -0.2493],\n",
              "         [ 0.3594,  0.3995, -1.5957],\n",
              "         [-1.5453,  0.6201, -0.7023],\n",
              "         [ 0.4077,  0.7170, -0.5429]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.unsqueeze(1) # 在第二维增加\n",
        "print(x.shape)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1f_1pYvCxAb",
        "outputId": "7f696722-a603-4e7d-a765-8f2083e38d1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 1, 4, 3])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[[-1.1183,  0.9595, -0.2493],\n",
              "           [ 0.3594,  0.3995, -1.5957],\n",
              "           [-1.5453,  0.6201, -0.7023],\n",
              "           [ 0.4077,  0.7170, -0.5429]]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.squeeze(0) # 减少第一维\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqYHa8ZzC1D7",
        "outputId": "fcb2a6b5-72a6-49cf-fbd1-10135ec46541"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.squeeze() # 将 tensor 中所有的一维全部都去掉\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDgww0KwDA0D",
        "outputId": "838b84cf-fbdd-4f32-a620-9ce467753ee6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4, 5)\n",
        "print(x.shape)\n",
        "\n",
        "# 使用permute和transpose进行维度交换\n",
        "x = x.permute(1, 0, 2) # permute 可以重新排列 tensor 的维度\n",
        "print(x.shape)\n",
        "\n",
        "x = x.transpose(0, 2)  # transpose 交换 tensor 中的两个维度\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUYMf3aHDDGd",
        "outputId": "f5d2e113-72ca-47f9-fbf3-60a75b225fb2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 5])\n",
            "torch.Size([4, 3, 5])\n",
            "torch.Size([5, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用 view 对 tensor 进行 reshape\n",
        "x = torch.randn(3, 4, 5)\n",
        "print(x.shape)\n",
        "\n",
        "x = x.view(-1, 5) # -1 表示任意的大小，5 表示第二维变成 5\n",
        "print(x.shape)\n",
        "\n",
        "x = x.view(3, 20) # 重新 reshape 成 (3, 20) 的大小\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dABb9JYvDFfO",
        "outputId": "72d97d92-a22a-42a6-adcd-a2a823e3f28e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4, 5])\n",
            "torch.Size([12, 5])\n",
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间"
      ],
      "metadata": {
        "id": "GtR3QzGUDaZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(3, 3)\n",
        "print(x.shape)\n",
        "\n",
        "# unsqueeze 进行 inplace\n",
        "x.unsqueeze_(0)\n",
        "print(x.shape)\n",
        "\n",
        "# transpose 进行 inplace\n",
        "x.transpose_(1, 0)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQVmSGjlDOc_",
        "outputId": "06f5f387-fe2c-4853-8231-1760e29b8ad1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3])\n",
            "torch.Size([1, 3, 3])\n",
            "torch.Size([3, 1, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(3, 3)\n",
        "y = torch.ones(3, 3)\n",
        "print(x)\n",
        "\n",
        "# add 进行 inplace\n",
        "x.add_(y)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeGTiYdrDcr_",
        "outputId": "f7410b56-4716-411a-802e-ff51d9e08832"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[2., 2., 2.],\n",
            "        [2., 2., 2.],\n",
            "        [2., 2., 2.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x+y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPxlPdzPDgEB",
        "outputId": "721f9123-4f7b-4c24-e556-228f938b5e23"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 3., 3.],\n",
              "        [3., 3., 3.],\n",
              "        [3., 3., 3.]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## variable"
      ],
      "metadata": {
        "id": "ReJaNkauDq18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensor 是 PyTorch 中的完美组件，但是构建神经网络还远远不够，我们需要能够构建计算图的 tensor，这就是 Variable。Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身.data，对应 tensor 的梯度.grad以及这个 Variable 是通过什么方式得到的.grad_fn"
      ],
      "metadata": {
        "id": "dbzbVAInDtA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过下面这种方式导入 Variable\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_tensor = torch.randn(10, 5)\n",
        "y_tensor = torch.randn(10, 5)\n",
        "\n",
        "# 将 tensor 变成 Variable\n",
        "x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度\n",
        "y = Variable(y_tensor, requires_grad=True)"
      ],
      "metadata": {
        "id": "nPJyOOrnDhZl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.sum(x + y)\n",
        "print(z.data)\n",
        "print(z.grad_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NvicOAWDvW2",
        "outputId": "317e6018-1cd0-49f8-d98b-313819090bc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-5.5970)\n",
            "<SumBackward0 object at 0x7f2316ea8790>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上面我们打出了 z 中的 tensor 数值，同时通过grad_fn知道了其是通过 Sum 这种方式得到的"
      ],
      "metadata": {
        "id": "6SSYwLudEKtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 求 x 和 y 的梯度\n",
        "z.backward()\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioLAc8gJELHb",
        "outputId": "8ce6e9ca-f7f2-4297-f5f7-8832f2ba4f3f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## autograd"
      ],
      "metadata": {
        "id": "gkq4_1P2EtyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Variable(torch.Tensor([2]), requires_grad=True)\n",
        "y = x + 2\n",
        "z = y ** 2 + 3\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqkR0PLrEOv3",
        "outputId": "fe920447-ae55-409d-c0b7-f171566bcf7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([19.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用自动求导\n",
        "z.backward()\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soDdBBAuEwWK",
        "outputId": "e0e57b54-9a44-43d6-aa3b-c2b774171168"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = Variable(torch.FloatTensor([[2, 3]]), requires_grad=True) # 构建一个 1 x 2 的矩阵\n",
        "n = Variable(torch.zeros(1, 2)) # 构建一个相同大小的 0 矩阵\n",
        "print(m)\n",
        "print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaszZ4vNE0zL",
        "outputId": "11e4cae5-9448-428b-bc6d-e741c88831a9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 3.]], requires_grad=True)\n",
            "tensor([[0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过 m 中的值计算新的 n 中的值\n",
        "n[0, 0] = m[0, 0] ** 2\n",
        "n[0, 1] = m[0, 1] ** 3\n",
        "print(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02HRmcNyFHHs",
        "outputId": "4415d66a-94ab-44cd-d3b3-fe2165e11c74"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 4., 27.]], grad_fn=<CopySlices>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "将上面的式子写成数学公式，可以得到\n",
        "n = (n0,n1) = (m0^2, m1^3)\n",
        "\n",
        "在 PyTorch 中，如果要调用自动求导，需要往backward()中传入一个参数，这个参数的形状和 n 一样大，比如是 (w0,w1)，那么自动求导的结果就是"
      ],
      "metadata": {
        "id": "1i7kbDNaFgJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n.backward(torch.ones_like(n)) # 将 (w0, w1) 取成 (1, 1)"
      ],
      "metadata": {
        "id": "4Nny2oLvFJWl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过调用 backward 我们可以进行一次自动求导，如果我们再调用一次 backward，会发现程序报错，没有办法再做一次。这是因为 PyTorch 默认做完一次自动求导之后，计算图就被丢弃了，所以两次自动求导需要手动设置一个东西，我们通过下面的小例子来说明。"
      ],
      "metadata": {
        "id": "O_H2uJr5F5EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
        "y = x * 2 + x ** 2 + 3\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvHUIvRlFN_9",
        "outputId": "2b5e4e72-a30f-4ef4-d43b-3041f3573969"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([18.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward(retain_graph=True) # 设置 retain_graph 为 True 来保留计算图"
      ],
      "metadata": {
        "id": "sHjCva1HF7Mu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MvS4b0pF9S8",
        "outputId": "7cc02c9c-1a27-4c3c-8b91-2a0a55c3fa1e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([8.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() # 再做一次自动求导，这次不保留计算图"
      ],
      "metadata": {
        "id": "XwuBB_5vF96U"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytciMc6NF99a",
        "outputId": "5cf15c39-ea7e-4916-f7ca-ef345419cedf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([16.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "可以发现 x 的梯度变成了 16，因为这里做了两次自动求导，所以讲第一次的梯度 8 和第二次的梯度 8 加起来得到了 16 的结果。"
      ],
      "metadata": {
        "id": "XIMpItIeGFIv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS8bJD0GF-Cf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}